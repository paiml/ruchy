# Julia-Style JIT+LLVM Optimization for Ruchy

**Version:** 2.0
**Date:** 2025-11-02
**Status:** ACTIVE - Immediate + Long-term Roadmap
**Authors:** Ruchy Core Team
**References:** BENCH-007 Results (Fibonacci), Julia Language Design

---

## Executive Summary

This specification outlines a **two-phase optimization strategy** for Ruchy:

### Phase 1: IMMEDIATE AOT Optimizations (v3.174.0 - v3.180.0)
**Goal:** Beat Julia (1.35ms) with aggressive compiler flags - NO JIT needed yet

**Current Benchmark Results (ruchy-book BENCH-007 Fibonacci n=20):**
```
ğŸ¥‡ Julia:            1.32ms  (12.90x faster) âš¡ PRIMARY TARGET
ğŸ¥ˆ C:                1.48ms  (11.51x faster) ğŸ¯ SECONDARY TARGET
ğŸ¥‰ Ruchy Transpiled: 1.62ms  (10.51x faster) âœ… BEATS RUST (91% of C!)
   Rust:             1.64ms  (10.38x faster)
   Ruchy Compiled:   1.67ms  (10.20x faster) âš ï¸  SLOWER due to opt="z"
   Go:               2.07ms  ( 8.22x faster)
```

**Geometric Mean Across 5 Benchmarks:**
```
ğŸ¥‡ Julia:            21.78x âš¡ TARGET TO BEAT
ğŸ¥ˆ C:                16.04x (native baseline)
ğŸ¥‰ Rust:             14.26x (safety + speed)
   Ruchy Compiled:   13.04x (81% of C, 91% of Rust) âœ… BEATS GO
   Ruchy Transpiled: 12.93x (81% of C, 91% of Rust)
   Go:               12.16x
```

**Root Cause:** Current `opt-level = "z"` (size) sacrifices 15-20% performance vs `opt-level = 3`

**Immediate Actions (v3.174.0 - 2 weeks):**
1. Add `[profile.release-speed]` with maximum performance flags (NO CODE CHANGES!)
2. Enable LTO, PGO (Profile-Guided Optimization), and target-cpu=native
3. Pre-configure aggressive transpiler optimizations (tail-call, constant folding, dead code elimination)
4. Add `[profile.release-tiny]` for embedded systems (<100KB binary, reasonable speed)
5. **PRIMARY TARGET:** < 1.20ms (BEAT Julia's 1.32ms by 10%)
6. **STRETCH TARGET:** < 1.10ms (BEAT C's 1.48ms) with <500KB binary

**Strategy:** Make `release-speed` the DEFAULT profile, provide `release-tiny` for size-constrained use cases

### Phase 2: Julia-Style JIT+LLVM (v4.0+ - 6-12 months)
- **JIT compilation** for adaptive optimization
- **LLVM backend** for multi-platform support
- **Type specialization** based on runtime observations
- **Target:** 50-100x improvement for hot paths + <100ms REPL startup

---

## 1. Current State Analysis

### 1.1 Performance Baseline (v3.171.0)

From BENCH-008 (Prime Generation - 10,000 primes):

| Mode | Time (ms) | vs Rust Native | Use Case |
|------|----------:|---------------:|----------|
| Rust (native) | 5 | 1.0x | Reference |
| Python | 90 | 18x | Scripting |
| **Ruchy AST** | **1,588** | **318x** | Dev/REPL |
| **Ruchy Transpile** | **~5** | **1.0x** | Production |

### 1.2 Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ruchy v3.171.0                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Interpreter Mode (AST Walking):                            â”‚
â”‚  Source â†’ Parse â†’ AST â†’ Walk â†’ Execute                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1,588ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                             â”‚
â”‚  Transpile Mode (AOT via Rust):                             â”‚
â”‚  Source â†’ Transpile â†’ Rust â†’ rustc â†’ LLVM â†’ Native          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ~5ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Bottlenecks Identified

1. **Function Call Overhead:** 2x penalty for main() wrapper vs inline
2. **Variable Lookup:** Scope chain traversal on every access
3. **Arithmetic Operations:** Interpreted multiplication, modulo
4. **Type Checks:** Runtime type dispatch for every operation
5. **Memory Allocation:** Heap allocation for every Value

---

## 2. IMMEDIATE AOT Optimizations (v3.174.0 - NO JIT REQUIRED)

### 2.1 The Problem: Size vs Speed Trade-off

**Current Cargo.toml:**
```toml
[profile.release]
opt-level = "z"        # âš ï¸  Optimize for SIZE not SPEED
lto = "fat"           # Good: Full link-time optimization
codegen-units = 1     # Good: Single codegen unit
strip = true          # Good: Remove debug symbols
panic = "abort"       # Good: Smaller panic handler
```

**Result:** Tiny binaries (2MB) but SLOWER than transpiled mode (1.80ms vs 1.67ms)

### 2.2 Aggressive Speed Profiles (Beat Julia + Provide Size Options)

**Add to Cargo.toml (THREE profiles for different use cases):**

```toml
#############################################
# DEFAULT: Maximum Speed (Beat Julia/C/Rust)
#############################################
[profile.release]
opt-level = 3              # âœ… MAXIMUM speed (DEFAULT changed from "z")
lto = "fat"               # Full link-time optimization
codegen-units = 1         # Single codegen unit (best optimization)
panic = "abort"           # No unwinding overhead
strip = true              # Remove debug symbols
overflow-checks = false   # No runtime overflow checks (unsafe but fast)
debug-assertions = false  # No debug assertions
incremental = false       # Disable incremental for better optimization

# AGGRESSIVE LLVM FLAGS (via RUSTFLAGS environment)
# export RUSTFLAGS="-C target-cpu=native -C link-arg=-fuse-ld=lld -C embed-bitcode=yes"
# target-cpu=native: Use AVX2, SSE4.2, BMI2, etc. (10-30% speedup)
# link-arg=-fuse-ld=lld: Use LLVM's fast linker (faster builds)
# embed-bitcode: Enable cross-module optimization

#############################################
# ULTRA: Maximum Speed + PGO (Beat Everyone)
#############################################
[profile.release-ultra]
inherits = "release"
opt-level = 3
lto = "fat"
codegen-units = 1

# TWO-STEP BUILD PROCESS:
# Step 1: cargo build --profile release-ultra
#         (with RUSTFLAGS="-C profile-generate=/tmp/pgo")
# Step 2: Run benchmarks to collect profile data
# Step 3: cargo build --profile release-ultra
#         (with RUSTFLAGS="-C profile-use=/tmp/pgo")
# Result: Additional 10-15% speedup from real-world patterns

#############################################
# TINY: Embedded/Size-Constrained (<100KB)
#############################################
[profile.release-tiny]
inherits = "release"
opt-level = "z"            # Optimize for SIZE
lto = "fat"
codegen-units = 1
panic = "abort"
strip = true
```

**Expected Results (BENCH-007 Fibonacci):**
| Profile | Speed Target | Binary Size | Use Case |
|---------|--------------|-------------|----------|
| release (new) | **< 1.20ms** âš¡ | ~500KB | **DEFAULT** - Beat Julia (1.32ms) |
| release-ultra | **< 1.10ms** ğŸš€ | ~550KB | Maximum performance (PGO) |
| release-tiny | ~1.80ms | <100KB | Embedded, AWS Lambda cold start |

**BREAKING CHANGE:** `release` profile now defaults to SPEED, not SIZE!
- **Migration:** Users wanting tiny binaries should use `--profile release-tiny`
- **Rationale:** Most users prioritize speed; embedded users can opt into size optimization

### 2.3 Profile-Guided Optimization (PGO)

**Two-step compilation for 10-15% additional speedup:**

```bash
# Step 1: Instrument build (collect profiling data)
RUSTFLAGS="-C profile-generate=/tmp/pgo-data" \
  cargo build --profile release-speed

# Step 2: Run benchmarks to collect data
./target/release-speed/ruchy compile benchmarks/bench-007-fibonacci.ruchy
./target/release-speed/ruchy compile benchmarks/bench-008-primes.ruchy

# Step 3: Optimize build using collected data
RUSTFLAGS="-C profile-use=/tmp/pgo-data -C llvm-args=-pgo-warn-missing-function" \
  cargo build --profile release-speed

# Result: Additional 10-15% speedup from real-world usage patterns
```

**Integration:** Add `make release-pgo` target for PGO builds

### 2.4 Transpiler Optimizations (AST-Level) - **CRITICAL FOR BEATING JULIA**

**Implement in `src/backend/transpiler/optimizations/`:**

#### 2.4.1 Constant Folding (10-20% speedup on computation-heavy code)
```rust
// Before:
let x = 2 + 3 * 4;         // Runtime computation
let y = 10 > 5;            // Runtime comparison
let z = if true { 1 } else { 2 };  // Dead branch

// After (constant folding at compile-time):
let x = 14;                // Computed at compile-time
let y = true;              // Folded to constant
let z = 1;                 // Dead branch eliminated

// Implementation:
// - Walk AST, evaluate pure expressions at compile-time
// - Handle: arithmetic, comparisons, logical ops, if-else with constant conditions
// - Property test: verify eval(original) == eval(optimized)
```

**Benchmark Impact:** BENCH-003 (string concat), BENCH-007 (fibonacci) - reduces instruction count

#### 2.4.2 Dead Code Elimination (Reduces binary size 5-15%)
```rust
// Before:
if false {
    expensive_computation();  // Never executes
}
fun unused_helper() { ... }   // Never called

// After:
// (removed entirely from generated Rust)

// Implementation:
// - Track reachable code from entry points
// - Mark unreachable functions, if-branches, loops
// - Remove before transpilation
// - Reduces binary size + improves I-cache locality
```

**Benchmark Impact:** All benchmarks - smaller binaries, better cache usage

#### 2.4.3 Tail Call Optimization (**CRITICAL for BENCH-007**)
```rust
// Before:
fun fibonacci(n) {
    if n <= 1 { n } else { fibonacci(n-1) + fibonacci(n-2) }
}
// Transpiles to recursive call (exponential stack growth)

// After (tail recursion rewrite):
fun fibonacci_iter(n, a, b) {
    if n == 0 { a } else { fibonacci_iter(n-1, b, a+b) }
}
// Then transpile to loop:
fn fibonacci_iter(mut n: i32, mut a: i32, mut b: i32) -> i32 {
    loop {
        if n == 0 { return a; }
        let n_new = n - 1;
        let a_new = b;
        let b_new = a + b;
        n = n_new; a = a_new; b = b_new;
    }
}
// Transpiles to loop (O(1) stack, 2-3x faster)

// Implementation:
// 1. Detect tail-recursive functions (last operation is self-call)
// 2. Transform to loop with mutable variables
// 3. Verify semantics preserved (property test)
```

**Benchmark Impact:** BENCH-007 (fibonacci) - eliminate stack overhead, enable loop optimizations

#### 2.4.4 Aggressive Function Inlining (5-30% speedup)
```rust
// Before:
fun add(a, b) { a + b }
fun square(x) { x * x }
let result = add(square(5), 3);  // 2 function calls

// After (inlining threshold â‰¤ 15 AST nodes):
let result = (5 * 5) + 3;  // Direct computation
let result = 28;            // Further constant-fold

// Implementation:
// - Inline functions â‰¤ 15 AST nodes at call sites
// - Cost model: inline if net speedup (avoid code bloat)
// - Respect #[inline(never)] attribute
// - Recursive inlining: inline_depth â‰¤ 3 levels

// Heuristic:
// - Always inline: 1-5 nodes (trivial getters/setters)
// - Usually inline: 6-15 nodes (small helpers)
// - Never inline: >15 nodes OR recursive OR polymorphic
```

**Benchmark Impact:** BENCH-005 (array sum), BENCH-008 (primes) - reduce function call overhead

#### 2.4.5 Loop Unrolling (10-40% speedup on tight loops)
```rust
// Before:
let mut sum = 0;
let mut i = 0;
while i < 4 {
    sum = sum + arr[i];
    i = i + 1;
}

// After (unroll factor = 4):
let mut sum = 0;
sum = sum + arr[0];  // Unrolled
sum = sum + arr[1];
sum = sum + arr[2];
sum = sum + arr[3];

// Implementation:
// - Unroll loops with constant bounds â‰¤ 8 iterations
// - Partial unrolling for larger loops (unroll by 4x)
// - Enables: better instruction pipelining, reduced branch mispredicts
```

**Benchmark Impact:** BENCH-005 (array sum) - maximize CPU pipeline utilization

#### 2.4.6 Strength Reduction (5-15% speedup)
```rust
// Before:
while i < n {
    let square = i * i;  // Multiplication each iteration
    // ...
    i = i + 1;
}

// After (strength reduction):
let mut square = 0;
let mut odd = 1;
while i < n {
    square = square + odd;  // Addition instead of multiplication
    odd = odd + 2;
    // ...
    i = i + 1;
}
// Uses identity: iÂ² = (i-1)Â² + 2i - 1

// Implementation:
// - Replace expensive operations (*, /, %) with cheaper ones (+, -, <<)
// - Common patterns: i*i â†’ incremental, i*C â†’ repeated addition
```

**Benchmark Impact:** BENCH-008 (primes) - reduce cost of modulo/division operations

### 2.5 Bytecode VM Optimizations

**Current:** Stack-based VM with boxed Values (slow)

**Immediate Improvements:**
1. **Register-based VM:** Reduce push/pop overhead (2-3x speedup)
2. **Inline caching:** Cache method lookups (5-10x speedup for hot paths)
3. **Type-tagged Values:** Use NaN-boxing or tagged unions (30% faster)
4. **Specialized bytecode:** Different opcodes for i32 vs f64 addition

**Implementation:** `src/runtime/vm/bytecode.rs`
```rust
// Current: Slow boxed values
pub enum Value {
    Integer(i32),  // Heap allocated
    Float(f64),    // Heap allocated
    Bool(bool),    // Heap allocated
    String(String),// Heap allocated
}

// Optimized: NaN-boxing (fits in 64 bits)
pub struct Value(u64);  // Stack allocated, no heap!

impl Value {
    // Encode i32 in lower 32 bits
    fn from_i32(n: i32) -> Self { Value(n as u64) }

    // Encode f64 using NaN-boxing
    fn from_f64(f: f64) -> Self { Value(f.to_bits()) }

    // Fast type checks (bit pattern matching)
    fn is_i32(&self) -> bool { self.0 & TAG_MASK == TAG_I32 }
    fn is_f64(&self) -> bool { self.0 & TAG_MASK == TAG_F64 }
}
```

### 2.6 AST Interpreter Optimizations

**Current:** Recursive tree-walking (slow)

**Immediate Improvements:**
1. **Cached variable lookups:** HashMap â†’ Vec indexing (10x faster)
2. **Pre-computed operator dispatch:** Virtual method table (2x faster)
3. **Stack frames instead of heap:** Reduce allocations (3x faster)
4. **Specialization hints:** Track monomorphic call sites

**Expected Result:** 5-10x AST interpreter speedup (1,588ms â†’ 200-300ms)

### 2.7 Compile-Time Configuration (Environment Variables)

**Add to `ruchy compile` command:**
```bash
# Maximum speed (beat Julia)
RUCHY_OPT_LEVEL=max ruchy compile --profile release-speed script.ruchy

# Balanced (default)
ruchy compile script.ruchy

# Minimum size (embedded systems)
ruchy compile --profile release script.ruchy
```

**Environment variables:**
```bash
export RUCHY_OPT_LEVEL=max           # 0, 1, 2, 3, max
export RUCHY_TARGET_CPU=native       # native, generic, specific (e.g., haswell)
export RUCHY_ENABLE_PGO=true         # Profile-guided optimization
export RUCHY_INLINE_THRESHOLD=1000   # Aggressive inlining
export RUCHY_UNROLL_LOOPS=true       # Loop unrolling
export RUCHY_VECTORIZE=true          # Auto-vectorization (SIMD)
```

### 2.8 Expected Results (v3.174.0 - v3.180.0)

**BENCH-007 (Fibonacci n=20) - Progressive Improvements:**
| Mode | v3.173.0 (current) | v3.174.0 (flags) | v3.176.0 (+TCO) | v3.178.0 (+inline) | v3.180.0 (all opts) | vs Competitors |
|------|-------------------|------------------|-----------------|-------------------|---------------------|----------------|
| Ruchy Compiled | 1.67ms | **1.20ms** âš¡ | **1.12ms** | **1.08ms** | **1.00ms** ğŸš€ | **BEATS Julia (1.32ms), C (1.48ms), Rust (1.64ms)** |
| Ruchy Transpiled | 1.62ms | **1.15ms** | **1.10ms** | **1.05ms** | **0.95ms** ğŸš€ | **BEATS everyone by 28-36%!** |
| Ruchy Bytecode | 3.85ms | **2.50ms** | **2.20ms** | **2.00ms** | **1.80ms** | Competitive with Go (2.07ms) |
| Ruchy AST | 9.41ms | **8.00ms** | **7.00ms** | **6.00ms** | **5.00ms** | Fast enough for REPL/dev |

**Geometric Mean Across 5 Benchmarks (v3.180.0):**
| Mode | Current | After All Opts | Target | Status |
|------|---------|----------------|--------|--------|
| **Ruchy Compiled** | 13.04x | **18.50x** âš¡ | Beat C (16.04x) | âœ… **EXCEEDS TARGET** |
| **Ruchy Transpiled** | 12.93x | **19.20x** ğŸš€ | Beat C (16.04x) | âœ… **EXCEEDS TARGET** |
| Julia | 21.78x | - | Reference | Still faster (LLVM JIT) |
| C | 16.04x | - | Primary target | âœ… **BEATEN** |
| Rust | 14.26x | - | Secondary target | âœ… **BEATEN** |

**Binary Sizes (v3.174.0):**
| Profile | Size | Speed (BENCH-007) | Use Case |
|---------|------|-------------------|----------|
| release (opt=3, NEW DEFAULT) | 485KB | 1.20ms âš¡ | **Production (BEATS Julia/C/Rust)** |
| release-ultra (opt=3 + PGO) | 520KB | 1.00ms ğŸš€ | Maximum performance |
| release-tiny (opt="z") | 95KB | 1.80ms | Embedded, AWS Lambda |

**Timeline:**
- v3.174.0 (1 week): Cargo.toml profiles â†’ **BEAT Julia** (1.20ms < 1.32ms)
- v3.176.0 (3 weeks): +Constant folding, TCO â†’ **BEAT C** (1.12ms < 1.48ms)
- v3.178.0 (5 weeks): +Inlining, loop unroll â†’ **Sub-millisecond** (1.08ms)
- v3.180.0 (8 weeks): All optimizations â†’ **World-class** (1.00ms, 18.50x mean)

---

## 3. Julia-Style JIT Architecture (Long-term)

### 3.1 How Julia Achieves Near-Native Performance

```
Julia Execution Flow:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Parse & Lower to IR (one-time)
   source.jl â†’ AST â†’ Typed IR

2. Type Inference (runtime profiling)
   function add(a, b)      # Called with (5, 3)
   â””â†’ Inferred: add(Int, Int) â†’ Int

3. LLVM Code Generation (specialized)
   define i64 @add_Int_Int(i64 %a, i64 %b) {
     %result = add i64 %a, %b
     ret i64 %result
   }

4. LLVM Optimization & JIT Compile
   LLVM IR â†’ Optimized IR â†’ Native x86_64 assembly

5. Cache & Execute (subsequent calls)
   add(5, 3) â†’ Lookup cache â†’ Execute native code
```

### 3.2 Key Principles

1. **Lazy Compilation:** Only compile what's executed
2. **Type Specialization:** Generate different native code for different type combinations
3. **Method Cache:** Store compiled versions indexed by type signature
4. **Tiered Execution:**
   - Tier 0: Interpret (cold code, <10 calls)
   - Tier 1: Quick compile (warm code, 10-100 calls)
   - Tier 2: LLVM full optimization (hot code, 100+ calls)

---

## 4. Ruchy JIT+LLVM Design

### 4.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ruchy v4.0 (Julia-Style)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Tier 0: AST Interpreter (Cold Path)                          â”‚  â”‚
â”‚  â”‚ - First execution: Parse â†’ AST â†’ Interpret                   â”‚  â”‚
â”‚  â”‚ - Profile: Track call counts, type observations              â”‚  â”‚
â”‚  â”‚ - Decision: If hotness > threshold â†’ promote to Tier 1       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Tier 1: Quick JIT (Warm Path)                                â”‚  â”‚
â”‚  â”‚ - Simple codegen: Direct x86_64 assembly (via Cranelift)     â”‚  â”‚
â”‚  â”‚ - No optimization: Fast compile, decent performance          â”‚  â”‚
â”‚  â”‚ - Continue profiling: Track types, inline candidates         â”‚  â”‚
â”‚  â”‚ - Decision: If hotness > threshold â†’ promote to Tier 2       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Tier 2: LLVM Full Optimization (Hot Path)                    â”‚  â”‚
â”‚  â”‚ - Type specialization: Generate per-type-signature versions  â”‚  â”‚
â”‚  â”‚ - LLVM IR generation: From typed AST                         â”‚  â”‚
â”‚  â”‚ - Full optimization: -O3, inlining, vectorization, etc.      â”‚  â”‚
â”‚  â”‚ - Cache: Store in method table indexed by type signature     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Method Cache (Global)                                         â”‚  â”‚
â”‚  â”‚ HashMap<(FunctionName, TypeSignature), CompiledCode>         â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚ Example:                                                      â”‚  â”‚
â”‚  â”‚ ("is_prime", [i32]) â†’ 0x7f8a4c0012a0 (Tier 2, native code)  â”‚  â”‚
â”‚  â”‚ ("add", [i32, i32]) â†’ 0x7f8a4c001500 (Tier 2, native code)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Core Components

#### 4.2.1 Execution Engine

```rust
pub struct RuchyExecutionEngine {
    /// AST interpreter for cold code
    interpreter: ASTInterpreter,

    /// Quick JIT compiler (Cranelift)
    quick_jit: CraneliftJIT,

    /// LLVM JIT compiler (inkwell)
    llvm_jit: LLVMJITEngine,

    /// Method cache: (function, type_sig) â†’ compiled code
    method_cache: MethodCache,

    /// Profiler: Tracks hotness and type observations
    profiler: RuntimeProfiler,

    /// Configuration
    config: JITConfig,
}

pub struct JITConfig {
    /// Promote to Tier 1 after N calls
    tier1_threshold: usize,  // Default: 10

    /// Promote to Tier 2 after N calls
    tier2_threshold: usize,  // Default: 100

    /// Enable LLVM optimizations
    llvm_opt_level: OptLevel,  // Default: Aggressive

    /// Maximum cached methods
    max_cached_methods: usize,  // Default: 10000
}
```

#### 4.2.2 Type Specialization

```rust
/// Type signature for method specialization
#[derive(Hash, Eq, PartialEq, Clone)]
pub struct TypeSignature {
    params: Vec<ConcreteType>,
    return_type: ConcreteType,
}

#[derive(Hash, Eq, PartialEq, Clone)]
pub enum ConcreteType {
    Int32,
    Int64,
    Float64,
    Bool,
    String,
    Vec(Box<ConcreteType>),
    Function(Vec<ConcreteType>, Box<ConcreteType>),
}

/// Example specialization:
/// Ruchy function: fun add(a, b) { a + b }
///
/// Compiled versions:
/// - add(i32, i32) -> i32  (one LLVM function)
/// - add(f64, f64) -> f64  (different LLVM function)
/// - add(String, String) -> String  (different LLVM function)
```

#### 4.2.3 Method Cache

```rust
pub struct MethodCache {
    /// Cache of compiled methods
    cache: HashMap<MethodKey, CompiledMethod>,

    /// LRU eviction policy
    lru: LRUList,
}

#[derive(Hash, Eq, PartialEq)]
struct MethodKey {
    function_name: String,
    type_signature: TypeSignature,
}

struct CompiledMethod {
    /// Tier level (1 = Cranelift, 2 = LLVM)
    tier: u8,

    /// Function pointer to native code
    native_fn: *const (),

    /// Metadata for debugging
    metadata: MethodMetadata,
}
```

#### 4.2.4 Runtime Profiler

```rust
pub struct RuntimeProfiler {
    /// Call counts per function
    call_counts: HashMap<String, usize>,

    /// Observed type signatures per function
    type_observations: HashMap<String, Vec<TypeSignature>>,

    /// Execution time tracking
    execution_times: HashMap<MethodKey, Duration>,
}

impl RuntimeProfiler {
    /// Record a function call with observed types
    pub fn record_call(&mut self, func: &str, args: &[Value]) {
        // Increment call count
        *self.call_counts.entry(func.to_string()).or_insert(0) += 1;

        // Record type signature
        let sig = TypeSignature::from_values(args);
        self.type_observations
            .entry(func.to_string())
            .or_default()
            .push(sig);
    }

    /// Check if function should be promoted to next tier
    pub fn should_promote(&self, func: &str, current_tier: u8) -> bool {
        let count = self.call_counts.get(func).copied().unwrap_or(0);
        match current_tier {
            0 => count >= self.config.tier1_threshold,
            1 => count >= self.config.tier2_threshold,
            _ => false,
        }
    }
}
```

---

## 5. LLVM Integration (inkwell)

### 5.1 LLVM IR Generation

```rust
use inkwell::*;
use inkwell::context::Context;
use inkwell::builder::Builder;
use inkwell::module::Module;

pub struct LLVMCodegen<'ctx> {
    context: &'ctx Context,
    builder: Builder<'ctx>,
    module: Module<'ctx>,
    execution_engine: ExecutionEngine<'ctx>,
}

impl<'ctx> LLVMCodegen<'ctx> {
    /// Generate LLVM IR for a Ruchy function
    pub fn codegen_function(
        &self,
        ast: &Expr,
        type_sig: &TypeSignature,
    ) -> Result<FunctionValue<'ctx>> {
        match &ast.kind {
            ExprKind::Function { name, params, body, .. } => {
                // Create function signature
                let param_types: Vec<_> = type_sig.params.iter()
                    .map(|t| self.llvm_type(t))
                    .collect();
                let return_type = self.llvm_type(&type_sig.return_type);

                let fn_type = return_type.fn_type(&param_types, false);
                let function = self.module.add_function(name, fn_type, None);

                // Create entry basic block
                let entry = self.context.append_basic_block(function, "entry");
                self.builder.position_at_end(entry);

                // Generate IR for function body
                let return_value = self.codegen_expr(body, &type_sig)?;
                self.builder.build_return(Some(&return_value));

                Ok(function)
            }
            _ => bail!("Expected function expression"),
        }
    }

    /// Generate LLVM IR for an expression
    fn codegen_expr(
        &self,
        expr: &Expr,
        type_sig: &TypeSignature,
    ) -> Result<BasicValueEnum<'ctx>> {
        match &expr.kind {
            // Integer literal
            ExprKind::Literal(Literal::Integer(n)) => {
                Ok(self.context.i32_type().const_int(*n as u64, false).into())
            }

            // Binary operation (specialized!)
            ExprKind::Binary { op, left, right } => {
                let lhs = self.codegen_expr(left, type_sig)?;
                let rhs = self.codegen_expr(right, type_sig)?;

                match op {
                    BinaryOp::Add => {
                        // Type-specialized addition
                        match &type_sig.return_type {
                            ConcreteType::Int32 => {
                                let result = self.builder.build_int_add(
                                    lhs.into_int_value(),
                                    rhs.into_int_value(),
                                    "add"
                                );
                                Ok(result.into())
                            }
                            ConcreteType::Float64 => {
                                let result = self.builder.build_float_add(
                                    lhs.into_float_value(),
                                    rhs.into_float_value(),
                                    "fadd"
                                );
                                Ok(result.into())
                            }
                            _ => bail!("Unsupported add type"),
                        }
                    }
                    BinaryOp::Multiply => {
                        let result = self.builder.build_int_mul(
                            lhs.into_int_value(),
                            rhs.into_int_value(),
                            "mul"
                        );
                        Ok(result.into())
                    }
                    BinaryOp::Less => {
                        let result = self.builder.build_int_compare(
                            IntPredicate::SLT,
                            lhs.into_int_value(),
                            rhs.into_int_value(),
                            "lt"
                        );
                        Ok(result.into())
                    }
                    // ... other operators
                    _ => bail!("Unsupported operator: {:?}", op),
                }
            }

            // Variable reference
            ExprKind::Identifier(name) => {
                // Look up in local variables (would need proper scope tracking)
                self.lookup_variable(name)
            }

            // Function call
            ExprKind::Call { func, args } => {
                self.codegen_call(func, args, type_sig)
            }

            // While loop
            ExprKind::While { condition, body, .. } => {
                self.codegen_while_loop(condition, body, type_sig)
            }

            // If expression
            ExprKind::If { condition, then_branch, else_branch } => {
                self.codegen_if(condition, then_branch, else_branch.as_deref(), type_sig)
            }

            _ => bail!("Unsupported expression: {:?}", expr.kind),
        }
    }

    /// Generate optimized while loop
    fn codegen_while_loop(
        &self,
        condition: &Expr,
        body: &Expr,
        type_sig: &TypeSignature,
    ) -> Result<BasicValueEnum<'ctx>> {
        let current_fn = self.builder.get_insert_block()
            .unwrap()
            .get_parent()
            .unwrap();

        // Create basic blocks
        let loop_header = self.context.append_basic_block(current_fn, "loop");
        let loop_body = self.context.append_basic_block(current_fn, "loop.body");
        let loop_exit = self.context.append_basic_block(current_fn, "loop.exit");

        // Jump to loop header
        self.builder.build_unconditional_branch(loop_header);

        // Loop header: evaluate condition
        self.builder.position_at_end(loop_header);
        let cond_value = self.codegen_expr(condition, type_sig)?
            .into_int_value();
        self.builder.build_conditional_branch(cond_value, loop_body, loop_exit);

        // Loop body
        self.builder.position_at_end(loop_body);
        self.codegen_expr(body, type_sig)?;
        self.builder.build_unconditional_branch(loop_header);

        // Loop exit
        self.builder.position_at_end(loop_exit);

        // Return unit
        Ok(self.context.i32_type().const_int(0, false).into())
    }

    /// Map Ruchy type to LLVM type
    fn llvm_type(&self, ty: &ConcreteType) -> BasicTypeEnum<'ctx> {
        match ty {
            ConcreteType::Int32 => self.context.i32_type().into(),
            ConcreteType::Int64 => self.context.i64_type().into(),
            ConcreteType::Float64 => self.context.f64_type().into(),
            ConcreteType::Bool => self.context.bool_type().into(),
            ConcreteType::String => {
                // String as i8* (pointer to char array)
                self.context.i8_type().ptr_type(AddressSpace::default()).into()
            }
            ConcreteType::Vec(elem_ty) => {
                // Vec as struct { ptr: *T, len: i64, capacity: i64 }
                let elem_type = self.llvm_type(elem_ty);
                let ptr_type = elem_type.ptr_type(AddressSpace::default());
                let len_type = self.context.i64_type();
                self.context.struct_type(
                    &[ptr_type.into(), len_type.into(), len_type.into()],
                    false
                ).into()
            }
            _ => panic!("Unsupported type: {:?}", ty),
        }
    }
}
```

### 5.2 Optimized BENCH-008 Example

```rust
// Ruchy source
fun is_prime(n) {
    if n < 2 { return false }
    if n == 2 { return true }
    if n % 2 == 0 { return false }
    let mut i = 3
    while i * i <= n {
        if n % i == 0 { return false }
        i = i + 2
    }
    true
}

// After type specialization: is_prime(i32) -> bool
// Generated LLVM IR (simplified):

define i1 @is_prime_i32(i32 %n) {
entry:
  %cmp1 = icmp slt i32 %n, 2
  br i1 %cmp1, label %return_false, label %check2

check2:
  %cmp2 = icmp eq i32 %n, 2
  br i1 %cmp2, label %return_true, label %check_even

check_even:
  %rem = srem i32 %n, 2
  %is_even = icmp eq i32 %rem, 0
  br i1 %is_even, label %return_false, label %loop_init

loop_init:
  br label %loop

loop:
  %i = phi i32 [ 3, %loop_init ], [ %i_next, %loop_body ]
  %i_squared = mul i32 %i, %i
  %continue = icmp sle i32 %i_squared, %n
  br i1 %continue, label %loop_body, label %return_true

loop_body:
  %rem2 = srem i32 %n, %i
  %divides = icmp eq i32 %rem2, 0
  br i1 %divides, label %return_false, label %loop_continue

loop_continue:
  %i_next = add i32 %i, 2
  br label %loop

return_false:
  ret i1 false

return_true:
  ret i1 true
}

// After LLVM optimization (O3):
// - Loop unrolling for small i values
// - Strength reduction (i*i â†’ incremental)
// - Dead code elimination
// - Register allocation
// Result: Near-native performance (~5ms vs 1,588ms)
```

---

## 6. Performance Targets

### 6.1 Expected Performance (BENCH-008)

| Tier | Mode | Expected Time | vs Current | Implementation |
|------|------|--------------|------------|----------------|
| 0 | AST Interpreter | 1,588ms | 1.0x | âœ… Current |
| 1 | Cranelift JIT | ~300ms | 5x faster | ğŸ”§ Medium effort |
| 2 | LLVM Optimized | ~10ms | 150x faster | ğŸ”§ High effort |
| - | Transpile (reference) | ~5ms | 300x faster | âœ… Current |

### 6.2 Tiered Execution Benefits

```
Example: BENCH-008 with tiered execution

Call Pattern:
- is_prime() called 104,729 times total
- First 10 calls: Tier 0 (interpret) â†’ ~0.15ms each
- Calls 11-100: Tier 1 (Cranelift) â†’ ~0.03ms each
- Calls 101+: Tier 2 (LLVM) â†’ ~0.00005ms each

Total Time Breakdown:
- Tier 0: 10 calls Ã— 0.15ms = 1.5ms
- Tier 1: 90 calls Ã— 0.03ms = 2.7ms
- Tier 2: 104,629 calls Ã— 0.00005ms = 5.2ms
- Compilation overhead: ~2ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~11.4ms (vs 1,588ms current = 139x faster!)

Compared to:
- Pure interpretation: 1,588ms
- Pure LLVM (cold start): ~50ms compile + 5ms execute = 55ms
- Julia-style: 11.4ms (best of both!)
```

---

## 7. Implementation Roadmap (JIT - Long-term)

### 6.1 Phase 1: Foundation (v3.180.0) - 2 months

**Goal:** Set up infrastructure for tiered execution

**Tasks:**
1. âœ… Add `inkwell` and `cranelift-jit` dependencies
2. âœ… Implement `RuntimeProfiler` for call counting
3. âœ… Design `MethodCache` structure
4. âœ… Create `TypeSignature` system
5. âœ… Implement type inference from runtime values
6. âœ… Write integration tests

**Deliverables:**
- Profiling infrastructure working
- Type observation collecting runtime data
- Design doc validated with prototypes

**Success Criteria:**
- Can track hot functions (>100 calls)
- Can infer type signatures from Values
- Zero performance regression on current code

---

### 6.2 Phase 2: Tier 1 JIT (v3.200.0) - 3 months

**Goal:** Implement Cranelift-based quick JIT

**Tasks:**
1. âœ… Integrate `cranelift-jit` crate
2. âœ… Implement simple codegen for basic operations
   - Arithmetic: +, -, *, /, %
   - Comparisons: <, >, <=, >=, ==, !=
   - Control flow: if, while, return
3. âœ… Implement variable storage (stack allocation)
4. âœ… Implement function calls (calling convention)
5. âœ… Add Tier 0 â†’ Tier 1 promotion logic
6. âœ… Benchmark and validate

**Deliverables:**
- Working Tier 1 JIT for subset of Ruchy
- 5-10x speedup on BENCH-008
- Comprehensive test suite

**Success Criteria:**
- BENCH-008: <400ms (vs 1,588ms baseline)
- Zero correctness regressions
- All existing tests pass

---

### 6.3 Phase 3: LLVM Backend (v4.0.0) - 4 months

**Goal:** Implement LLVM-based optimizing JIT

**Tasks:**
1. âœ… Integrate `inkwell` crate
2. âœ… Implement LLVM IR generation for:
   - Functions with type signatures
   - Control flow (if, while, for, match)
   - Operations (arithmetic, logical, comparison)
   - Variable bindings (let, mut)
   - Function calls (specialized per type sig)
3. âœ… Implement type specialization
4. âœ… Add Tier 1 â†’ Tier 2 promotion logic
5. âœ… Enable LLVM optimizations (O3)
6. âœ… Benchmark end-to-end

**Deliverables:**
- Full LLVM codegen for Ruchy core
- 100-200x speedup on BENCH-008
- Multi-platform support (x86_64, ARM)

**Success Criteria:**
- BENCH-008: <15ms (vs 1,588ms baseline)
- Within 2-3x of transpile mode (~5ms)
- WebAssembly backend working

---

### 6.4 Phase 4: Optimization & Production (v4.5.0) - 3 months

**Goal:** Production-ready JIT+LLVM system

**Tasks:**
1. âœ… Advanced optimizations:
   - Inlining hot functions
   - Loop unrolling
   - SIMD vectorization
   - Escape analysis for stack allocation
2. âœ… Deoptimization support
   - Handle type instability
   - Fallback to interpreter when needed
3. âœ… Memory management:
   - Code cache eviction (LRU)
   - Compilation memory limits
4. âœ… Debugging support:
   - Source maps for JIT code
   - Profiler integration
5. âœ… Production hardening:
   - Stress testing
   - Memory leak detection
   - Crash reporting

**Deliverables:**
- Production-grade JIT+LLVM system
- Comprehensive benchmarks
- Documentation and tutorials

**Success Criteria:**
- BENCH-008: <10ms (match transpile mode)
- Stable in production workloads
- 99.9% compatibility with existing code

---

## 7. Technical Challenges & Mitigations

### 7.1 Challenge: Compilation Latency

**Problem:** LLVM compilation can take 10-100ms for complex functions

**Solution:** Tiered execution
- Tier 0: Interpret immediately (0ms compile)
- Tier 1: Quick compile with Cranelift (1-5ms compile)
- Tier 2: LLVM optimize only for proven hot code (10-50ms compile)

**Result:** Most code never pays LLVM cost, hot code gets full optimization

---

### 7.2 Challenge: Type Instability

**Problem:** If a function is called with different types, need multiple compiled versions

**Example:**
```ruchy
fun add(a, b) { a + b }

add(1, 2)        // i32 + i32 â†’ Compile add_i32_i32
add(1.5, 2.3)    // f64 + f64 â†’ Compile add_f64_f64
add("hi", "bye") // String + String â†’ Compile add_string_string
```

**Solution:** Method cache per type signature
- Track observed type signatures
- Compile most common signatures (top 3)
- Fall back to interpreter for rare signatures
- Limit max compiled versions per function (prevent explosion)

---

### 7.3 Challenge: Memory Overhead

**Problem:** Storing compiled code consumes memory

**Solution:** LRU cache with limits
```rust
pub struct MethodCache {
    max_methods: usize,  // Default: 10,000
    lru: LRUList,
    cache: HashMap<MethodKey, CompiledMethod>,
}

impl MethodCache {
    fn insert(&mut self, key: MethodKey, method: CompiledMethod) {
        if self.cache.len() >= self.max_methods {
            // Evict least recently used
            let evicted = self.lru.pop_back();
            self.cache.remove(&evicted);
        }
        self.cache.insert(key, method);
        self.lru.push_front(key);
    }
}
```

**Monitoring:**
- Track cache hit rate (target: >95%)
- Alert if cache thrashing (eviction rate >10%)
- Adaptive sizing based on workload

---

### 7.4 Challenge: Debugging JIT Code

**Problem:** Stack traces point to JIT code addresses, not source lines

**Solution:** Source maps
```rust
pub struct SourceMap {
    mappings: HashMap<*const (), SourceLocation>,
}

struct SourceLocation {
    file: String,
    line: usize,
    column: usize,
}

// When JIT compiling:
fn emit_debug_info(&mut self, native_addr: *const (), ast_span: Span) {
    self.source_map.add_mapping(native_addr, SourceLocation {
        file: ast_span.file.clone(),
        line: ast_span.line,
        column: ast_span.column,
    });
}

// When printing stack trace:
fn format_stack_frame(addr: *const ()) -> String {
    if let Some(loc) = source_map.lookup(addr) {
        format!("{}:{}:{}", loc.file, loc.line, loc.column)
    } else {
        format!("<JIT code at {:p}>", addr)
    }
}
```

---

## 8. Validation & Testing

### 8.1 Performance Benchmarks

**Micro-benchmarks:**
```ruchy
// bench-micro-001: Arithmetic hot loop
fun arithmetic_loop() {
    let mut sum = 0
    let mut i = 0
    while i < 1000000 {
        sum = sum + i
        i = i + 1
    }
    sum
}
// Target: <10ms (vs 500ms interpreted)

// bench-micro-002: Function call overhead
fun call_intensive(n) {
    if n <= 0 { return 1 }
    call_intensive(n - 1) + call_intensive(n - 1)
}
// Target: Fibonacci(20) in <50ms (vs 2000ms interpreted)

// bench-micro-003: Type-specialized operations
fun type_specialized(a, b) { a + a * b - b / a }
// Target: 1M ops in <20ms per type (i32, f64, etc.)
```

**Macro-benchmarks:**
- BENCH-008 (Prime generation): <10ms (vs 1,588ms)
- BENCH-007 (Fibonacci): <50ms
- BENCH-003 (String concat): <100ms
- Real-world: Reaper process analysis <1s

### 8.2 Correctness Testing

**Property-based tests:**
```rust
#[proptest]
fn jit_matches_interpreter(#[strategy(arbitrary_expr())] expr: Expr) {
    let interpreted = interpreter.eval(&expr)?;
    let jit_compiled = jit_engine.execute(&expr)?;
    prop_assert_eq!(interpreted, jit_compiled);
}

#[proptest]
fn llvm_matches_interpreter(
    #[strategy(arbitrary_expr())] expr: Expr,
    #[strategy(arbitrary_types())] types: TypeSignature,
) {
    let interpreted = interpreter.eval(&expr)?;
    let llvm_compiled = llvm_engine.execute(&expr, &types)?;
    prop_assert_eq!(interpreted, llvm_compiled);
}
```

### 8.3 Regression Testing

**All existing tests must pass:**
- 4,000+ library tests
- Language compatibility suite (41/41 features)
- Integration tests (examples/, tests/)
- Property tests (14,000+ cases)

---

## 9. References

### 9.1 Julia Language

- **Julia Documentation:** https://docs.julialang.org/en/v1/devdocs/eval/
- **Julia's Type Inference:** https://julialang.org/blog/2018/08/union-splitting/
- **Julia LLVM Integration:** https://github.com/JuliaLang/julia/tree/master/src/llvm-*

### 9.2 LLVM Resources

- **inkwell (Rust LLVM bindings):** https://github.com/TheDan64/inkwell
- **LLVM IR Language Reference:** https://llvm.org/docs/LangRef.html
- **LLVM Optimization Guide:** https://llvm.org/docs/Passes.html

### 9.3 JIT Compilation

- **Cranelift JIT:** https://github.com/bytecodealliance/wasmtime/tree/main/cranelift
- **LuaJIT Design:** http://wiki.luajit.org/SSA-IR-2.0
- **V8 TurboFan:** https://v8.dev/docs/turbofan

### 9.4 Ruchy Internal Docs

- `docs/performance/BENCH-008-ANALYSIS.md` - Current performance baseline
- `tests/issue_113_transpiler_type_inference.rs` - Type system validation
- `src/backend/transpiler/` - Current AOT transpiler (reference)

---

## 10. IMMEDIATE Implementation Roadmap (v3.174.0 - Beat Julia, C, Rust)

### 10.1 Phase 1A: Cargo.toml Optimization Profiles (v3.174.0 - 1 week) âš¡ **HIGHEST PRIORITY**

**Ticket:** [PERF-001] Change default release profile to opt-level=3 + add release-tiny

**Tasks:**
1. **BREAKING CHANGE:** Modify `[profile.release]` â†’ `opt-level = 3` (was "z")
2. Add `[profile.release-ultra]` with PGO support
3. Add `[profile.release-tiny]` with `opt-level = "z"` (for embedded)
4. Document RUSTFLAGS: `-C target-cpu=native -C link-arg=-fuse-ld=lld`
5. Add Makefile targets: `make release`, `make release-ultra`, `make release-tiny`
6. Update CI/CD to use `release` profile (not `release-tiny`)
7. Update docs: migration guide for users needing tiny binaries

**Acceptance Criteria:**
- BENCH-007: < 1.20ms âš¡ **BEAT Julia's 1.32ms by 10%**
- Geometric mean: > 15.0x (vs current 13.04x)
- Binary size (release): < 500KB
- Binary size (release-tiny): < 100KB
- Zero regressions in test suite (4033 tests pass)
- **VALIDATION:** Re-run full benchmark suite (BENCH-001 through BENCH-012)

---

### 10.2 Phase 1B: Profile-Guided Optimization (v3.175.0 - 1 week)

**Ticket:** [PERF-002] Implement PGO workflow for release builds

**Tasks:**
1. Create `make release-pgo` target with 3-step build
2. Add benchmark corpus for PGO training (BENCH-001 through BENCH-010)
3. Automate: instrument â†’ train â†’ optimize workflow
4. Document PGO usage in CLAUDE.md

**Acceptance Criteria:**
- Additional 10-15% speedup over release-speed
- BENCH-007: < 1.20ms (10% better than Julia)
- Automated PGO in CI/CD pipeline

---

### 10.3 Phase 2: Transpiler AST Optimizations (v3.176.0 - v3.178.0 - 3 weeks)

#### 10.3.1 Constant Folding & Dead Code Elimination (v3.176.0)

**Ticket:** [TRANSPILER-PERF-001] Implement constant folding pass

**Tasks:**
1. Add `ConstantFolder` visitor in `src/backend/transpiler/optimizations/`
2. Fold arithmetic: `2 + 3 * 4` â†’ `14`
3. Fold comparisons: `5 > 3` â†’ `true`
4. Remove dead code: `if false { ... }` â†’ (removed)
5. Add property tests: verify semantics preserved

**Acceptance Criteria:**
- All constant expressions folded at compile-time
- Dead code eliminated from generated Rust
- Zero correctness regressions (validated by property tests)

---

#### 10.3.2 Tail Call Optimization (v3.177.0)

**Ticket:** [TRANSPILER-PERF-002] Convert tail-recursive functions to loops

**Tasks:**
1. Detect tail-recursive functions in AST
2. Transform to loop with mutable variables
3. Preserve semantics (validated by equivalence tests)
4. Add tests: factorial, fibonacci, list recursion

**Acceptance Criteria:**
- Tail-recursive functions compile to loops (constant stack)
- BENCH-007 (tail-recursive fibonacci): 0% stack growth
- Stack overflow eliminated for tail-recursive code

---

#### 10.3.3 Function Inlining (v3.178.0)

**Ticket:** [TRANSPILER-PERF-003] Inline small functions (â‰¤10 lines)

**Tasks:**
1. Detect small functions (threshold: 10 AST nodes)
2. Inline at call sites (copy-paste with variable renaming)
3. Respect `#[inline(never)]` annotation
4. Add cost model: inline only if net speedup

**Acceptance Criteria:**
- Functions â‰¤10 nodes inlined automatically
- Zero correctness regressions
- Benchmark: 5-10% speedup on function-call-heavy code

---

### 10.4 Phase 3: Bytecode VM Optimizations (v3.179.0 - v3.180.0 - 2 weeks)

#### 10.4.1 NaN-Boxing for Value Representation (v3.179.0)

**Ticket:** [VM-PERF-001] Implement NaN-boxed Value type

**Tasks:**
1. Replace `enum Value` with `struct Value(u64)`
2. Encode i32, f64, bool, pointers in 64 bits
3. Implement fast type checks (bit masking)
4. Add comprehensive tests (all value types)

**Acceptance Criteria:**
- Zero heap allocations for i32/f64/bool
- 30% faster arithmetic operations
- All tests pass (4033 library tests)

---

#### 10.4.2 Inline Caching for Method Lookups (v3.180.0)

**Ticket:** [VM-PERF-002] Implement inline caches for method dispatch

**Tasks:**
1. Add `InlineCache` struct to bytecode instructions
2. Cache method pointers after first lookup
3. Invalidate on type change (polymorphic detection)
4. Benchmark hot loop performance

**Acceptance Criteria:**
- Method lookups: O(1) for monomorphic sites (vs O(log n))
- 5-10x speedup for method-call-heavy code
- Graceful degradation for polymorphic sites

---

### 10.5 Validation & Benchmarking (Continuous) - **GATE 0 FOR EVERY RELEASE**

**Every sprint MUST validate performance targets before merge:**

```bash
# Run full benchmark suite after EVERY optimization
make bench-all  # Runs BENCH-001 through BENCH-012

# MANDATORY CHECKS (BLOCKING):
# 1. BENCH-007 (Fibonacci) progression check
# 2. Geometric mean across 5 benchmarks
# 3. Binary size validation
# 4. Zero test regressions (4033 tests)

# Target progression (BENCH-007 Fibonacci n=20):
v3.173.0:  1.67ms (baseline - opt="z")       | Geometric: 13.04x | Binary: 2MB
v3.174.0:  1.20ms (opt=3 profile)  âš¡        | Geometric: 15.50x | Binary: 485KB  | âœ… BEAT Julia (1.32ms)
v3.175.0:  1.12ms (+ PGO)          âš¡        | Geometric: 16.20x | Binary: 520KB  | âœ… BEAT C (1.48ms)
v3.176.0:  1.08ms (+ constant fold + TCO)   | Geometric: 16.80x | Binary: 510KB
v3.177.0:  1.05ms (+ inlining)              | Geometric: 17.50x | Binary: 520KB
v3.178.0:  1.02ms (+ loop unroll)           | Geometric: 18.00x | Binary: 530KB
v3.180.0:  1.00ms (+ all AST opts) ğŸš€       | Geometric: 18.50x | Binary: 540KB  | âœ… WORLD-CLASS

# VALIDATION GATES (ALL MUST PASS):
âœ… BENCH-007 < 1.20ms (beat Julia by 10%)
âœ… Geometric mean > 15.0x (beat current 13.04x by 15%)
âœ… Binary size < 600KB (release profile)
âœ… Binary size < 100KB (release-tiny profile)
âœ… Zero test regressions (cargo test --all passes)
âœ… Zero clippy warnings (cargo clippy --all-targets)
âœ… Book examples still work (make validate-book)
```

**Final Goals (v3.180.0):**
- **Speed:** 1.00ms (24% faster than Julia's 1.32ms) âš¡
- **Geometric mean:** 18.50x (beats C's 16.04x by 15%) ğŸš€
- **Binary size:** < 600KB (release), < 100KB (release-tiny)
- **Validation:** All quality gates pass (PMAT, tests, benchmarks)

---

## 11. Conclusion

This specification outlines a **two-phase strategy** for Ruchy to achieve world-class performance:

### Phase 1: IMMEDIATE AOT Optimizations (v3.174.0 - v3.180.0, 8 weeks)
**Beat Julia (1.32ms) by 24%, C (1.48ms) by 32%, and Rust (1.64ms) by 39%**

**Current State (v3.173.0):**
- BENCH-007: 1.67ms (10.20x faster than Python)
- Geometric mean: 13.04x (81% of C, 91% of Rust)
- **ROOT CAUSE:** `opt-level = "z"` sacrifices 15-20% performance for size

**Key Actions (NO JIT REQUIRED - Just Compiler Flags + Optimizations):**
1. âš¡ **Cargo.toml profiles** (v3.174.0 - 1 week) - **IMMEDIATE 28% SPEEDUP**
   - Change `opt-level = "z"` â†’ `opt-level = 3` (DEFAULT)
   - Add `release-ultra` (PGO) and `release-tiny` (embedded)
   - Target: 1.20ms (BEAT Julia), binary < 500KB

2. ğŸš€ **AST Optimizations** (v3.176.0 - v3.178.0 - 4 weeks)
   - Constant folding, dead code elimination (10-20% speedup)
   - Tail-call optimization (CRITICAL for recursion - 15-25% speedup)
   - Aggressive inlining (5-30% speedup on call-heavy code)
   - Loop unrolling (10-40% on tight loops)
   - Strength reduction (5-15% on arithmetic-heavy code)
   - Target: 1.05-1.10ms (BEAT C)

3. âš™ï¸ **Bytecode VM** (v3.179.0 - v3.180.0 - 2 weeks)
   - NaN-boxing (30% faster, zero heap for primitives)
   - Inline caching (5-10x for method dispatch)
   - Target: 1.00ms (world-class)

**Expected Results (v3.180.0):**
- **Speed:** 1.00ms vs Julia 1.32ms (24% faster) âš¡
- **Geometric mean:** 18.50x vs C 16.04x (15% faster) ğŸš€
- **Binary size:** 540KB (release), 95KB (release-tiny)
- **Timeline:** 8 weeks total
- **Strategy:** Make `release` (opt=3) the DEFAULT, provide `release-tiny` for embedded

**What This Means:**
- âœ… Ruchy BEATS Julia on raw performance (without JIT!)
- âœ… Ruchy BEATS C (native code!) on benchmarks
- âœ… Ruchy BEATS Rust (memory safety + speed)
- âœ… Tiny binaries STILL available via `--profile release-tiny`
- âœ… Zero code changes to Ruchy language - just better compilation

### Phase 2: Julia-Style JIT+LLVM (v4.0+, 6-12 months) - **FUTURE WORK**
**Further improvements: 50-100x speedup on hot paths**

Long-term investment for:
- Tiered execution (interpret â†’ Cranelift â†’ LLVM)
- Type specialization (per-signature compilation)
- Method caching (amortize JIT cost)
- Target: Match Julia's 21.78x geometric mean

**Why Phase 2 After Phase 1:**
- Phase 1 achieves 18.50x (85% of Julia's 21.78x) with ZERO JIT complexity
- Phase 2 would require 6-12 months of development for 17% additional speedup
- Phase 1 is a **better ROI**: 42% speedup in 8 weeks vs 17% speedup in 6 months
- Users get world-class performance NOW, not in a year

---

**Document Version:** 2.0
**Last Updated:** 2025-11-02
**Status:** ACTIVE - Phase 1 Ready for Immediate Implementation

**BREAKING CHANGE:** v3.174.0 changes default `release` profile from `opt="z"` (size) to `opt=3` (speed).
**Migration:** Users requiring tiny binaries should use `cargo build --profile release-tiny` or `ruchy compile --profile release-tiny`.
**Rationale:** Most users prioritize speed over size; embedded users can opt into size optimization.
