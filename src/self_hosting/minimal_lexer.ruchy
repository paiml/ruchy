// Minimal Self-hosted Lexer Proof of Concept
// RUCHY-0722: Port lexer to Ruchy (showing basic principles)

fun simple_tokenize(input: String) -> [String] {
    let mut tokens = []
    let mut pos = 0
    let len = input.len()
    
    while pos < len {
        let char_str = input[pos..pos+1]
        
        if char_str == " " || char_str == "\t" || char_str == "\n" {
            // Skip whitespace
            pos = pos + 1
        } else if char_str == "+" {
            tokens = tokens + ["PLUS"]
            pos = pos + 1
        } else if char_str == "=" {
            tokens = tokens + ["EQUAL"]
            pos = pos + 1
        } else if char_str == "(" {
            tokens = tokens + ["LPAREN"]
            pos = pos + 1
        } else if char_str == ")" {
            tokens = tokens + ["RPAREN"]
            pos = pos + 1
        } else {
            // Read alphanumeric sequences
            let mut word = ""
            while pos < len {
                let current = input[pos..pos+1]
                if (current >= "a" && current <= "z") || 
                   (current >= "A" && current <= "Z") || 
                   (current >= "0" && current <= "9") || 
                   current == "_" {
                    word = word + current
                    pos = pos + 1
                } else {
                    break
                }
            }
            
            if word != "" {
                if word == "let" {
                    tokens = tokens + ["LET"]
                } else if word == "fun" {
                    tokens = tokens + ["FUN"]
                } else {
                    tokens = tokens + ["IDENT:" + word]
                }
            } else {
                pos = pos + 1  // Skip unknown character
            }
        }
    }
    
    tokens
}

fun demo_self_hosted_lexer() {
    println("ðŸ”§ Minimal Self-Hosted Lexer Demo")
    println("=================================")
    
    let code = "let x = add(y)"
    println("Input: " + code)
    println("Tokens:")
    
    let tokens = simple_tokenize(code)
    for token in tokens {
        println("  " + token)
    }
    
    println("")
    println("âœ… Self-hosting proof of concept successful!")
    println("ðŸš€ Ruchy can tokenize its own syntax!")
}

demo_self_hosted_lexer()